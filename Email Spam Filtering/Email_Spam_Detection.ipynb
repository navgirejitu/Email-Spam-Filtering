{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('emails.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  spam\n",
       "0  Subject: naturally irresistible your corporate...     1\n",
       "1  Subject: the stock trading gunslinger  fanny i...     1\n",
       "2  Subject: unbelievable new homes made easy  im ...     1\n",
       "3  Subject: 4 color printing special  request add...     1\n",
       "4  Subject: do not have money , get software cds ...     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesssing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5728, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shape of the data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'spam'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing duplicates\n",
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5695, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text    0\n",
       "spam    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check for missing data\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      text\n",
       "spam      \n",
       "0     4327\n",
       "1     1368"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('spam').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "label_counts = df.spam.value_counts()\n",
    "plt.figure(figsize = (12,6))\n",
    "sns.barplot(label_counts.index, label_counts.values, alpha = 0.9)\n",
    "\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.xlabel('Spam', fontsize =12)\n",
    "plt.ylabel('Counts', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stopword package\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    #Remove the punctuations\n",
    "    nopunc = [char for char in text if char not in string.punctuation]\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    #Remove Strowords\n",
    "    clean_words = [word for word in  nopunc.split() if word.lower() not in stopwords.words('english') ]\n",
    "    \n",
    "    #Return list of clean text words\n",
    "    return clean_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocess(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_data_copy = df['text'].copy()\n",
    "message_data_copy = message_data_copy.apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Subject naturally irresistible corporate ident...\n",
      "1       Subject stock trading gunslinger fanny merrill...\n",
      "2       Subject unbelievable new homes made easy im wa...\n",
      "3       Subject 4 color printing special request addit...\n",
      "4       Subject money get software cds software compat...\n",
      "                              ...                        \n",
      "5723    Subject research development charges gpg forwa...\n",
      "5724    Subject receipts visit jim thanks invitation v...\n",
      "5725    Subject enron case study update wow day super ...\n",
      "5726    Subject interest david please call shirley cre...\n",
      "5727    Subject news aurora 5 2 update aurora version ...\n",
      "Name: text, Length: 5695, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(message_data_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5695x37187 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 544384 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer  = TfidfVectorizer(\"english\")\n",
    "message_mat = vectorizer.fit_transform(message_data_copy)\n",
    "message_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 18513)\t0.054968610144776595\n",
      "  (0, 26345)\t0.07957984862389081\n",
      "  (0, 20966)\t0.04871593365622061\n",
      "  (0, 9062)\t0.09542942112354177\n",
      "  (0, 28842)\t0.07687470062065854\n",
      "  (0, 21055)\t0.08450081604335585\n",
      "  (0, 32580)\t0.11150747315175288\n",
      "  (0, 14502)\t0.09355701220828218\n",
      "  (0, 14170)\t0.08743474156436504\n",
      "  (0, 8322)\t0.06757655429861795\n",
      "  (0, 4751)\t0.07531090141784935\n",
      "  (0, 34762)\t0.09407155679063696\n",
      "  (0, 27189)\t0.057621043616563\n",
      "  (0, 16371)\t0.0852669849992569\n",
      "  (0, 29821)\t0.09330538783473592\n",
      "  (0, 343)\t0.0707078170067817\n",
      "  (0, 7384)\t0.08911307909854094\n",
      "  (0, 15554)\t0.10601861627492867\n",
      "  (0, 7144)\t0.0869097477767971\n",
      "  (0, 4206)\t0.11080386674538054\n",
      "  (0, 10830)\t0.0598372702106354\n",
      "  (0, 33525)\t0.0691632651130315\n",
      "  (0, 36390)\t0.059659027014386704\n",
      "  (0, 12265)\t0.10947736905165323\n",
      "  (0, 30181)\t0.046120867476797854\n",
      "  :\t:\n",
      "  (5694, 31543)\t0.0842243374332838\n",
      "  (5694, 28778)\t0.036931750575061005\n",
      "  (5694, 10825)\t0.047186370161168056\n",
      "  (5694, 24384)\t0.01908723779599118\n",
      "  (5694, 9139)\t0.04857921788506317\n",
      "  (5694, 14423)\t0.024389679448985194\n",
      "  (5694, 24568)\t0.02687073424086279\n",
      "  (5694, 35349)\t0.12232431779585974\n",
      "  (5694, 3959)\t0.029041304709050066\n",
      "  (5694, 31498)\t0.03224573895851995\n",
      "  (5694, 15055)\t0.029104123512177837\n",
      "  (5694, 35543)\t0.026683526233063436\n",
      "  (5694, 5434)\t0.029536364798062622\n",
      "  (5694, 23557)\t0.08188869460062205\n",
      "  (5694, 26345)\t0.036013047699201495\n",
      "  (5694, 8319)\t0.030480107692948434\n",
      "  (5694, 34996)\t0.04739520752973595\n",
      "  (5694, 12140)\t0.030606527951891626\n",
      "  (5694, 21263)\t0.027668019281589316\n",
      "  (5694, 12598)\t0.0405977597201485\n",
      "  (5694, 21398)\t0.02151967406147721\n",
      "  (5694, 35985)\t0.029877028512659855\n",
      "  (5694, 18208)\t0.0986501110307898\n",
      "  (5694, 21621)\t0.07606340367029252\n",
      "  (5694, 32301)\t0.007799959227912188\n"
     ]
    }
   ],
   "source": [
    "print(message_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemmer (text):\n",
    "    text = text.split()\n",
    "    words = \"\"\n",
    "    for i in text:\n",
    "            stemmer = SnowballStemmer(\"english\")\n",
    "            words += (stemmer.stem(i))+\" \"\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>spam</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Subject: naturally irresistible your corporate...</td>\n",
       "      <td>1</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Subject: the stock trading gunslinger  fanny i...</td>\n",
       "      <td>1</td>\n",
       "      <td>598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Subject: unbelievable new homes made easy  im ...</td>\n",
       "      <td>1</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Subject: 4 color printing special  request add...</td>\n",
       "      <td>1</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Subject: do not have money , get software cds ...</td>\n",
       "      <td>1</td>\n",
       "      <td>235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Subject: great nnews  hello , welcome to medzo...</td>\n",
       "      <td>1</td>\n",
       "      <td>478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Subject: here ' s a hot play in motion  homela...</td>\n",
       "      <td>1</td>\n",
       "      <td>9340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Subject: save your money buy getting this thin...</td>\n",
       "      <td>1</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Subject: undeliverable : home based business f...</td>\n",
       "      <td>1</td>\n",
       "      <td>507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Subject: save your money buy getting this thin...</td>\n",
       "      <td>1</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Subject: las vegas high rise boom  las vegas i...</td>\n",
       "      <td>1</td>\n",
       "      <td>709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Subject: save your money buy getting this thin...</td>\n",
       "      <td>1</td>\n",
       "      <td>446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Subject: brighten those teeth  get your  teeth...</td>\n",
       "      <td>1</td>\n",
       "      <td>879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Subject: wall street phenomenon reaps rewards ...</td>\n",
       "      <td>1</td>\n",
       "      <td>8196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Subject: fpa notice : ebay misrepresentation o...</td>\n",
       "      <td>1</td>\n",
       "      <td>684</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  spam  length\n",
       "0   Subject: naturally irresistible your corporate...     1    1484\n",
       "1   Subject: the stock trading gunslinger  fanny i...     1     598\n",
       "2   Subject: unbelievable new homes made easy  im ...     1     448\n",
       "3   Subject: 4 color printing special  request add...     1     500\n",
       "4   Subject: do not have money , get software cds ...     1     235\n",
       "5   Subject: great nnews  hello , welcome to medzo...     1     478\n",
       "6   Subject: here ' s a hot play in motion  homela...     1    9340\n",
       "7   Subject: save your money buy getting this thin...     1     446\n",
       "8   Subject: undeliverable : home based business f...     1     507\n",
       "9   Subject: save your money buy getting this thin...     1     446\n",
       "10  Subject: las vegas high rise boom  las vegas i...     1     709\n",
       "11  Subject: save your money buy getting this thin...     1     446\n",
       "12  Subject: brighten those teeth  get your  teeth...     1     879\n",
       "13  Subject: wall street phenomenon reaps rewards ...     1    8196\n",
       "14  Subject: fpa notice : ebay misrepresentation o...     1     684"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length'] = df['text'].apply(len)\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = df['length'].to_numpy()\n",
    "new_mat = np.hstack((message_mat.todense(),length[:, None]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By Using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "message_train, message_test, spam_nospam_train, spam_nospam_test = train_test_split(message_mat, \n",
    "                                                        df['spam'], test_size=0.3, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.973083674663546"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Spam_model = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "Spam_model.fit(message_train, spam_nospam_train)\n",
    "pred = Spam_model.predict(message_test)\n",
    "accuracy_score(spam_nospam_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t3\n",
      "  (0, 4)\t2\n",
      "  (0, 2)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 3)\t4\n",
      "  (1, 1)\t1\n"
     ]
    }
   ],
   "source": [
    "#matrix of token count\n",
    "msgX = 'hello hello world hello world play'\n",
    "msgY = 'test test test test one hello'\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#from sklearn.pipeline import Pipeline\n",
    "\n",
    "#vectorizer = TfidfVectorizer(\"english\")\n",
    "\n",
    "#message_mat = vectorizer.fit_transform([[msgX],[msgY]])\n",
    "#message_mat\n",
    "\n",
    "bowXY = CountVectorizer(analyzer=process_text).fit_transform([[msgX],[msgY]])\n",
    "print(bowXY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bowXY.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#bow_msgs = vectorizer.fit_transform(message_data_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting collecction of text to matriz token count \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "bow_messages = CountVectorizer(analyzer=process_text).fit_transform(message_data_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3638)\t1\n",
      "  (0, 23369)\t1\n",
      "  (0, 18841)\t1\n",
      "  (0, 10065)\t1\n",
      "  (0, 17696)\t1\n",
      "  (0, 21140)\t1\n",
      "  (0, 27986)\t1\n",
      "  (0, 16674)\t1\n",
      "  (0, 28110)\t1\n",
      "  (0, 9296)\t3\n",
      "  (0, 21654)\t2\n",
      "  (0, 15429)\t1\n",
      "  (0, 32602)\t1\n",
      "  (0, 18238)\t1\n",
      "  (0, 18886)\t1\n",
      "  (0, 16089)\t2\n",
      "  (0, 8054)\t1\n",
      "  (0, 20952)\t3\n",
      "  (0, 32319)\t1\n",
      "  (0, 31968)\t1\n",
      "  (0, 24838)\t1\n",
      "  (0, 36025)\t2\n",
      "  (0, 21431)\t2\n",
      "  (0, 33037)\t1\n",
      "  (0, 23040)\t2\n",
      "  :\t:\n",
      "  (5694, 24818)\t2\n",
      "  (5694, 21624)\t1\n",
      "  (5694, 5729)\t9\n",
      "  (5694, 30934)\t1\n",
      "  (5694, 2828)\t3\n",
      "  (5694, 13338)\t1\n",
      "  (5694, 13127)\t1\n",
      "  (5694, 17388)\t1\n",
      "  (5694, 14130)\t1\n",
      "  (5694, 20273)\t1\n",
      "  (5694, 31827)\t1\n",
      "  (5694, 13128)\t1\n",
      "  (5694, 20467)\t1\n",
      "  (5694, 35288)\t1\n",
      "  (5694, 8629)\t1\n",
      "  (5694, 30082)\t1\n",
      "  (5694, 13522)\t5\n",
      "  (5694, 36185)\t1\n",
      "  (5694, 959)\t2\n",
      "  (5694, 2797)\t1\n",
      "  (5694, 30287)\t1\n",
      "  (5694, 17590)\t1\n",
      "  (5694, 33923)\t1\n",
      "  (5694, 10373)\t1\n",
      "  (5694, 11386)\t1\n"
     ]
    }
   ],
   "source": [
    "print(bow_messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(bow_messages,df['spam'],test_size=0.20,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5695, 37229)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_messages.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_train, bow_test, bspam_nospam_train, bspam_nospam_test = train_test_split(bow_messages, \n",
    "                                                        df['spam'], test_size=0.3, random_state=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9853715623171445"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "Spam_model = LogisticRegression(solver='liblinear', penalty='l1')\n",
    "Spam_model.fit(bow_train, bspam_nospam_train)\n",
    "pred = Spam_model.predict(bow_test)\n",
    "accuracy_score(bspam_nospam_test,pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(df[\"text\"],df[\"spam\"], test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4556,)\n",
      "(1139,)\n",
      "(4556,)\n",
      "(1139,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(stop_words='english')"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect = CountVectorizer(stop_words='english')\n",
    "vect.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0000', '00000000', '000000000003619', '000000000003991', '000000000005168', '000000000005409', '000000000005411', '000000000005413', '000000000005820', '000000000006238', '000000000006452', '000000000007494', '000000000007498', '000000000007876', '000000000010552', '000000000011185', '000000000012735', '000000000012736']\n",
      "['zunaechst', 'zunf', 'zur', 'zurich', 'zusaetzlich', 'zuzana', 'zwabic', 'zwischen', 'zwlaszcza', 'zwrocic', 'zwwyw', 'zwzm', 'zxghlajf', 'zyban', 'zyc', 'zygoma', 'zymg', 'zzn', 'zzncacst', 'zzzz']\n"
     ]
    }
   ],
   "source": [
    "print(vect.get_feature_names()[0:20])\n",
    "print(vect.get_feature_names()[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_df = vect.transform(X_train)\n",
    "X_test_df = vect.transform(X_test)\n",
    "type(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = dict()\n",
    "# Naive Bayes Machine Learning Model\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train_df,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[\"naive_bayes\"] = model.predict(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.990342405618964"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,prediction[\"naive_bayes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         Ham       0.99      0.99      0.99       852\n",
      "        Spam       0.98      0.98      0.98       287\n",
      "\n",
      "    accuracy                           0.99      1139\n",
      "   macro avg       0.99      0.99      0.99      1139\n",
      "weighted avg       0.99      0.99      0.99      1139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, prediction['naive_bayes'], target_names = [\"Ham\", \"Spam\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type here something and see if it belongs to spam: You could be entitled up to Â£3,160 in compensation from mis-sold PPI on a credit card or loan. Please reply PPI for info or STOP to opt out.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'DataFrame' has no attribute 'from_items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-ecc819e26ab0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Type here something and see if it belongs to spam: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'spam'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-81-ecc819e26ab0>\u001b[0m in \u001b[0;36mclassify\u001b[1;34m(user_input)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mcustom_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0muser_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcustom_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mtext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcustom_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: type object 'DataFrame' has no attribute 'from_items'"
     ]
    }
   ],
   "source": [
    "def classify(user_input):\n",
    "    custom_train = [('text', [user_input])]\n",
    "    custom_train = pd.DataFrame.from_items(custom_train)\n",
    "    text = custom_train.iloc[:, 0].values\n",
    "    return model.predict(vectorize.transform(text))\n",
    "\n",
    "text = input('Type here something and see if it belongs to spam: ')\n",
    "if classify(text):\n",
    "    print('spam')\n",
    "else:\n",
    "    print('ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Dell/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-----------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-083e45b2b070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3968\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3969\u001b[0m         \"\"\"\n\u001b[1;32m-> 3970\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3971\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[0;32m   3972\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"map\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m   1158\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1159\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1160\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1161\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1162\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-82-083e45b2b070>\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(text)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokens'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     return [\n\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[1;32m--> 106\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    751\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 752\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    753\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"raw\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"nltk\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    878\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"file\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"*\"\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 585\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Dell/nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Dell\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
